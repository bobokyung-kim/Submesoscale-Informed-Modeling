{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f0b5f-89ab-44a2-8256-54956faba1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b8b24-0255-421d-863d-3e006176261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import torchvision.transforms.functional as TF\n",
    "import re\n",
    "from torch_dct import dct_2d, idct_2d\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading functions\n",
    "def load_data_from_mat(filename, u_v_names=None, mld_name=None):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if u_v_names:\n",
    "            u_velocity = torch.tensor(np.array(f[u_v_names[0]])).float()  # [t, n, m]\n",
    "            v_velocity = torch.tensor(np.array(f[u_v_names[1]])).float()  # [t, n, m]\n",
    "        else:\n",
    "            u_velocity = v_velocity = None\n",
    "\n",
    "        if mld_name:\n",
    "            mld = torch.tensor(np.array(f[mld_name])).squeeze().float()  # [t]\n",
    "        else:\n",
    "            mld = None\n",
    "\n",
    "    return u_velocity, v_velocity, mld\n",
    "\n",
    "def load_multiple_h5py_data(u_v_files_pattern, u_v_names):\n",
    "    u_velocity_list, v_velocity_list = [], []\n",
    "    u_v_files = glob.glob(u_v_files_pattern)\n",
    "    u_v_files = sorted(u_v_files, key=lambda x: [int(num) for num in re.findall(r'\\d+', x)])\n",
    "    \n",
    "    for file in u_v_files:\n",
    "        u_velocity, v_velocity = load_data_from_mat(file, u_v_names, None)[:2]\n",
    "        u_velocity_list.append(u_velocity)\n",
    "        v_velocity_list.append(v_velocity)\n",
    "\n",
    "    u_velocity = torch.cat(u_velocity_list, dim=0)  # [t_total, n, m]\n",
    "    v_velocity = torch.cat(v_velocity_list, dim=0)  # [t_total, n, m]\n",
    "\n",
    "    return u_velocity, v_velocity\n",
    "\n",
    "def load_multiple_mld_data(mld_files_pattern, mld_name):\n",
    "    mld_list = []\n",
    "    mld_files = glob.glob(mld_files_pattern)\n",
    "    for file in mld_files:\n",
    "        _, _, mld = load_data_from_mat(file, None, mld_name)\n",
    "        if mld is not None:\n",
    "            mld_list.append(mld)\n",
    "    if len(mld_list) == 0:\n",
    "        raise ValueError(\"No MLD data found.\")\n",
    "    mld = torch.cat(mld_list, dim=0)  # [t_total]\n",
    "    return mld\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = tensor.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    std = tensor.std(dim=(0, 2, 3), keepdim=True)\n",
    "    return (tensor - mean) / (std + 1e-5)\n",
    "\n",
    "class LRVelocityMLDDataset(Dataset):\n",
    "    def __init__(self, low_u_velocity, low_v_velocity, mld, inter_u_velocity, inter_v_velocity, normalize_fn=None):\n",
    "        self.low_u_velocity = torch.nan_to_num(low_u_velocity)  # [t, n, m]\n",
    "        self.low_v_velocity = torch.nan_to_num(low_v_velocity)  # [t, n, m]\n",
    "        self.mld = torch.nan_to_num(mld)                # [t]\n",
    "        self.inter_u_velocity = torch.nan_to_num(inter_u_velocity)  # [t, n, m]\n",
    "        self.inter_v_velocity = torch.nan_to_num(inter_v_velocity)  # [t, n, m]\n",
    "\n",
    "        self.inputs = torch.stack([self.low_u_velocity, self.low_v_velocity, self.mld], dim=1)  # [t, 4, n, m]\n",
    "        self.outputs = torch.stack([self.inter_u_velocity, self.inter_v_velocity], dim=1)  # [t, 2, n, m]\n",
    "\n",
    "        if normalize_fn:\n",
    "            self.inputs = normalize_fn(self.inputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx, :, :, :]                  # [4, n, m]\n",
    "        output_data = self.outputs[idx, :, :, :]                  # [2, n, m]\n",
    "        return input_data, output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b557c5-a7db-4d93-8bf7-b55376b76c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, low_u_velocity, low_v_velocity, mld, inter_u_velocity, inter_v_velocity, normalize_fn=None):\n",
    "        self.low_u_velocity = torch.nan_to_num(low_u_velocity)  # [t, n, m]\n",
    "        self.low_v_velocity = torch.nan_to_num(low_v_velocity)  # [t, n, m]\n",
    "        self.mld = torch.nan_to_num(mld)                # [t]\n",
    "        self.inter_u_velocity = torch.nan_to_num(inter_u_velocity)  # [t, n, m]\n",
    "        self.inter_v_velocity = torch.nan_to_num(inter_v_velocity)  # [t, n, m]\n",
    "\n",
    "        self.inputs = torch.stack([self.low_u_velocity, self.low_v_velocity, self.mld], dim=1)  # [t, 4, n, m]\n",
    "        self.outputs = torch.stack([self.inter_u_velocity, self.inter_v_velocity], dim=1)  # [t, 2, n, m]\n",
    "        \n",
    "        if normalize_fn:\n",
    "            self.inputs = normalize_fn(self.inputs)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx, :, :, :]\n",
    "        output_data = self.outputs[idx, :, :]\n",
    "        return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85499a01-516c-4b02-9f0f-2f3e58c57dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEMO_MODE:\n",
    "    low_u_v_names_train = ['low_BBE_U_surf', 'low_BBE_V_surf']\n",
    "    inter_u_v_names_train = ['inv_dct_band_surf_u', 'inv_dct_band_surf_v']\n",
    "    mld_name_train = 'low_mld'\n",
    "    \n",
    "    train_low_u_velocity, train_low_v_velocity = load_multiple_h5py_data(\n",
    "        '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/downsampling_roms_veloicty_rho_points_month_*.mat',\n",
    "        low_u_v_names_train\n",
    "    )\n",
    "    print(f\"train_low_u_velocity shape: {train_low_u_velocity.shape}\")  # [t_total]\n",
    "    print(f\"train_low_v_velocity shape: {train_low_v_velocity.shape}\")  # [t_total]\n",
    "    \n",
    "    train_inter_u_velocity, train_inter_v_velocity = load_multiple_h5py_data(\n",
    "        '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/intermediate_surface_velocity_rho_points_month_*.mat',\n",
    "        inter_u_v_names_train\n",
    "    )\n",
    "    print(f\"train_inter_u_velocity shape: {train_inter_u_velocity.shape}\")  # [t_total]\n",
    "    print(f\"train_inter_v_velocity shape: {train_inter_v_velocity.shape}\")  # [t_total]\n",
    "    \n",
    "    mld_files_pattern_train = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/downsampling_roms_spatial_mld_month_*.mat'\n",
    "    train_mld = load_multiple_mld_data(mld_files_pattern_train, mld_name_train)\n",
    "    print(f\"train_mld shape: {train_mld.shape}\")  # [t_total]\n",
    "    \n",
    "    assert train_low_u_velocity.shape[0] == train_low_v_velocity.shape[0] == train_mld.shape[0], \\\n",
    "        \"All input tensors must have the same number of samples (t dimension).\"\n",
    "    \n",
    "    assert train_inter_u_velocity.shape[0] == train_inter_v_velocity.shape[0], \\\n",
    "        \"All output tensors must have the same number of samples (t dimension).\"\n",
    "    \n",
    "    num_hours_per_month = [744, 672, 744, 720, 744, 720, 744, 744, 720, 744, 720, 744]\n",
    "    cumulative_hours = np.cumsum([0] + num_hours_per_month)\n",
    "    \n",
    "    input_train_low_u, input_val_low_u = [], []\n",
    "    input_train_low_v, input_val_low_v = [], []\n",
    "    input_train_mld, input_val_mld = [], []\n",
    "    output_train_inter_u, output_val_inter_u = [], []\n",
    "    output_train_inter_v, output_val_inter_v = [], []\n",
    "    \n",
    "    for i in range(12):\n",
    "        start, end = cumulative_hours[i], cumulative_hours[i+1]\n",
    "    \n",
    "        input_low_u_month = train_low_u_velocity[start:end]\n",
    "        input_low_v_month = train_low_v_velocity[start:end]\n",
    "        input_mld_month = train_mld[start:end]\n",
    "        output_inter_u_month = train_inter_u_velocity[start:end]\n",
    "        output_inter_v_month = train_inter_v_velocity[start:end]\n",
    "    \n",
    "        full_combined = list(zip(\n",
    "            input_low_u_month, input_low_v_month, input_mld_month,\n",
    "            output_inter_u_month, output_inter_v_month\n",
    "        ))\n",
    "    \n",
    "        train_combined, val_combined = train_test_split(full_combined, test_size=0.2, random_state=42)\n",
    "    \n",
    "        (low_u_train, low_v_train, mld_train, inter_u_train, inter_v_train) = zip(*train_combined)\n",
    "        (low_u_val, low_v_val, mld_val, inter_u_val, inter_v_val) = zip(*val_combined)\n",
    "        \n",
    "        input_train_low_u.append(np.stack(low_u_train))\n",
    "        input_val_low_u.append(np.stack(low_u_val))\n",
    "        input_train_low_v.append(np.stack(low_v_train))\n",
    "        input_val_low_v.append(np.stack(low_v_val))\n",
    "        input_train_mld.append(np.stack(mld_train))\n",
    "        input_val_mld.append(np.stack(mld_val))\n",
    "    \n",
    "        output_train_inter_u.append(np.stack(inter_u_train))\n",
    "        output_val_inter_u.append(np.stack(inter_u_val))\n",
    "        output_train_inter_v.append(np.stack(inter_v_train))\n",
    "        output_val_inter_v.append(np.stack(inter_v_val))\n",
    "        \n",
    "    inputs_low_u_train = np.concatenate(input_train_low_u)\n",
    "    inputs_low_u_val = np.concatenate(input_val_low_u)\n",
    "    inputs_low_v_train = np.concatenate(input_train_low_v)\n",
    "    inputs_low_v_val = np.concatenate(input_val_low_v)\n",
    "    inputs_mld_train = np.concatenate(input_train_mld)\n",
    "    inputs_mld_val = np.concatenate(input_val_mld)\n",
    "    \n",
    "    outputs_inter_u_train = np.concatenate(output_train_inter_u)\n",
    "    outputs_inter_u_val = np.concatenate(output_val_inter_u)\n",
    "    outputs_inter_v_train = np.concatenate(output_train_inter_v)\n",
    "    outputs_inter_v_val = np.concatenate(output_val_inter_v)\n",
    "    \n",
    "    print(f\"Train size: {len(inputs_low_u_train)} ({len(inputs_low_u_train) / 8760 * 100:.1f}%)\")\n",
    "    print(f\"Validation size: {len(outputs_inter_u_val)} ({len(outputs_inter_u_val) / 8760 * 100:.1f}%)\")\n",
    "    \n",
    "    inputs_low_u_train = torch.from_numpy(inputs_low_u_train).float()\n",
    "    inputs_low_u_val = torch.from_numpy(inputs_low_u_val).float()\n",
    "    inputs_low_v_train = torch.from_numpy(inputs_low_v_train).float()\n",
    "    inputs_low_v_val = torch.from_numpy(inputs_low_v_val).float()\n",
    "    inputs_mld_train = torch.from_numpy(inputs_mld_train).float()\n",
    "    inputs_mld_val = torch.from_numpy(inputs_mld_val).float()\n",
    "    \n",
    "    outputs_inter_u_train = torch.from_numpy(outputs_inter_u_train).float()\n",
    "    outputs_inter_u_val = torch.from_numpy(outputs_inter_u_val).float()\n",
    "    outputs_inter_v_train = torch.from_numpy(outputs_inter_v_train).float()\n",
    "    outputs_inter_v_val = torch.from_numpy(outputs_inter_v_val).float()\n",
    "    \n",
    "    train_dataset = LRVelocityMLDDataset(\n",
    "        low_u_velocity=inputs_low_u_train,\n",
    "        low_v_velocity=inputs_low_v_train,\n",
    "        mld=inputs_mld_train,\n",
    "        inter_u_velocity=outputs_inter_u_train,\n",
    "        inter_v_velocity=outputs_inter_v_train,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "    \n",
    "    val_dataset = LRVelocityMLDDataset(\n",
    "        low_u_velocity=inputs_low_u_val,\n",
    "        low_v_velocity=inputs_low_v_val,\n",
    "        mld=inputs_mld_val,\n",
    "        inter_u_velocity=outputs_inter_u_val,\n",
    "        inter_v_velocity=outputs_inter_v_val,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    low_u_v_names_test = ['hycom_u_surf_rotate', 'hycom_v_surf_rotate']\n",
    "    inter_u_v_names_test = ['inv_dct_band_surf_u', 'inv_dct_band_surf_v']\n",
    "    mld_name_test = 'mld_romsgrid'\n",
    "    \n",
    "    # Load the Coriolis parameter\n",
    "    hycom_file_path = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/surf_vertical_vorticity_HYCOM_month_1.mat'\n",
    "    \n",
    "    with h5py.File(hycom_file_path, 'r') as mat_file:\n",
    "        hycom_f = mat_file['hycom_f'][:]\n",
    "    \n",
    "    def load_test_data():\n",
    "        test_u_velocity, test_v_velocity = load_multiple_h5py_data(\n",
    "            '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/HYCOM_GLBy008_spatial_surf_velocity_rotate_ROMSgrid_2021_*.mat',\n",
    "            low_u_v_names_test\n",
    "        )\n",
    "    \n",
    "        mld_files_pattern_test = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/HYCOM_GLBy008_temp_ROMSgrid_2021_*.mat'\n",
    "        test_mld = load_multiple_mld_data(mld_files_pattern_test, mld_name_test)\n",
    "        print(f\"test_mld shape: {test_mld.shape}\")  # [t_total]\n",
    "    \n",
    "        test_inter_u_velocity, test_inter_v_velocity = load_multiple_h5py_data(\n",
    "            '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/intermediate_surface_velocity_rho_points_month_*.mat',\n",
    "            inter_u_v_names_test\n",
    "        )\n",
    "        \n",
    "        print(f\"test_u_velocity shape: {test_u_velocity.shape}\")  # [t_total, n, m]\n",
    "        print(f\"test_v_velocity shape: {test_v_velocity.shape}\")  # [t_total, n, m]\n",
    "        print(f\"test_mld shape: {test_mld.shape}\")                # [t_total]\n",
    "        print(f\"test_inter_u_velocity shape: {test_inter_u_velocity.shape}\")                # [t_total]\n",
    "        print(f\"test_inter_v_velocity shape: {test_inter_v_velocity.shape}\")                # [t_total]\n",
    "    \n",
    "        return test_u_velocity, test_v_velocity, test_mld, test_inter_u_velocity, test_inter_v_velocity\n",
    "    \n",
    "    test_u_velocity, test_v_velocity, test_mld, test_inter_u_velocity, test_inter_v_velocity = load_test_data()\n",
    "    \n",
    "    test_u_velocity = test_u_velocity[:2912, :, :]\n",
    "    test_v_velocity = test_v_velocity[:2912, :, :]\n",
    "    test_mld = test_mld[:2912, :, :]\n",
    "    test_inter_u_velocity = test_inter_u_velocity[0:8736:3, :, :]\n",
    "    test_inter_v_velocity = test_inter_v_velocity[0:8736:3, :, :]\n",
    "    \n",
    "    test_dataset = TestDataset(\n",
    "        low_u_velocity=test_u_velocity,\n",
    "        low_v_velocity=test_v_velocity,\n",
    "        mld=test_mld,\n",
    "        inter_u_velocity = test_inter_u_velocity,\n",
    "        inter_v_velocity = test_inter_v_velocity,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "else:\n",
    "    print(\"DEMO MODE: using dummy tensors (no external data files).\")\n",
    "\n",
    "    t_train = 16\n",
    "    t_val = 8\n",
    "    t_test = 8\n",
    "    n, m = 280, 340\n",
    "\n",
    "    inputs_low_u_train = torch.randn(t_train, n, m)\n",
    "    inputs_low_v_train = torch.randn(t_train, n, m)\n",
    "    inputs_mld_train_1d   = torch.randn(t_train)\n",
    "    outputs_inter_u_train = torch.randn(t_train, n, m)\n",
    "    outputs_inter_v_train = torch.randn(t_train, n, m)\n",
    "\n",
    "    inputs_low_u_val = torch.randn(t_val, n, m)\n",
    "    inputs_low_v_val = torch.randn(t_val, n, m)\n",
    "    inputs_mld_val_1d   = torch.randn(t_val)\n",
    "    outputs_inter_u_val = torch.randn(t_val, n, m)\n",
    "    outputs_inter_v_val = torch.randn(t_val, n, m)\n",
    "\n",
    "    inputs_mld_train = inputs_mld_train_1d[:, None, None].repeat(1, n, m)  # [t,n,m]\n",
    "    inputs_mld_val   = inputs_mld_val_1d[:, None, None].repeat(1, n, m)    # [t,n,m]\n",
    "\n",
    "    train_dataset = LRVelocityMLDDataset(\n",
    "        low_u_velocity=inputs_low_u_train,\n",
    "        low_v_velocity=inputs_low_v_train,\n",
    "        mld=inputs_mld_train,\n",
    "        inter_u_velocity=outputs_inter_u_train,\n",
    "        inter_v_velocity=outputs_inter_v_train,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "\n",
    "    val_dataset = LRVelocityMLDDataset(\n",
    "        low_u_velocity=inputs_low_u_val,\n",
    "        low_v_velocity=inputs_low_v_val,\n",
    "        mld=inputs_mld_val,\n",
    "        inter_u_velocity=outputs_inter_u_val,\n",
    "        inter_v_velocity=outputs_inter_v_val,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # ---- dummy test tensors ----\n",
    "    test_u_velocity = torch.randn(t_test, n, m)\n",
    "    test_v_velocity = torch.randn(t_test, n, m)\n",
    "    test_mld_1d        = torch.randn(t_test)   # [t]\n",
    "    test_inter_u_velocity = torch.randn(t_test, n, m)\n",
    "    test_inter_v_velocity = torch.randn(t_test, n, m)\n",
    "\n",
    "    test_mld = test_mld_1d[:, None, None].repeat(1, n, m)  # [t, n, m]\n",
    "\n",
    "    test_dataset = TestDataset(\n",
    "        low_u_velocity=test_u_velocity,\n",
    "        low_v_velocity=test_v_velocity,\n",
    "        mld=test_mld,\n",
    "        inter_u_velocity=test_inter_u_velocity,\n",
    "        inter_v_velocity=test_inter_v_velocity,\n",
    "        normalize_fn=normalize\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a88723-df78-42b9-bc91-1994fab0a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dct_bandpass_mask(H, W, dx=1.0, dy=1.0, lambda_min=5.0, lambda_max=20.0):\n",
    "    \"\"\"\n",
    "    H, W: height, width of the spatial grid (e.g., 256)\n",
    "    dx, dy: grid resolution in km (default 1.0 km)\n",
    "    lambda_min, lambda_max: passband in km\n",
    "    \"\"\"\n",
    "    mask = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    Lx = H * dx  # total domain size in x\n",
    "    Ly = W * dy  # total domain size in y\n",
    "\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            # Avoid division by zero (i = 0 or j = 0 are low frequency)\n",
    "            lambda_x = 2 * Lx / (2 * i + 1)\n",
    "            lambda_y = 2 * Ly / (2 * j + 1)\n",
    "\n",
    "            # Equivalent isotropic wavelength\n",
    "            lambda_eq = 1.0 / np.sqrt((1/lambda_x**2) + (1/lambda_y**2))\n",
    "\n",
    "            # Bandpass condition\n",
    "            if lambda_min <= lambda_eq <= lambda_max:\n",
    "                mask[i, j] = 1.0\n",
    "\n",
    "    # Convert to torch tensor and normalize shape\n",
    "    mask_tensor = torch.tensor(mask)  # shape: [H, W]\n",
    "    return mask_tensor\n",
    "\n",
    "def dual_loss(pred, target, freq_mask, alpha=0.95, beta=0.05):\n",
    "    pixel_loss = F.smooth_l1_loss(pred, target)\n",
    "\n",
    "    pred_band = idct_2d(dct_2d(pred.float()) * freq_mask.unsqueeze(0).unsqueeze(0))\n",
    "    target_band = idct_2d(dct_2d(target.float()) * freq_mask.unsqueeze(0).unsqueeze(0))\n",
    "    spec_loss = F.mse_loss(pred_band, target_band)\n",
    "\n",
    "    return alpha * pixel_loss + beta * spec_loss\n",
    "\n",
    "freq_mask = create_dct_bandpass_mask(H=280, W=340,\n",
    "                                      dx=1.0, dy=1.0,\n",
    "                                      lambda_min=5.0, lambda_max=20.0)\n",
    "freq_mask = freq_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99465fd0-e9b1-4e1b-b33b-04b62ee04eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60429cb-42e0-44e7-85cf-faa48862d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                p_outputs = model(inputs)\n",
    "                predictions.append(p_outputs.cpu().numpy())  # Store predictions\n",
    "                true_labels.append(outputs.cpu().numpy())  # Store true labels\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56902e-c290-46e3-887a-0599563fbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, accumulate_steps=4, device='cuda', scheduler=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = dual_loss(outputs, labels, freq_mask, alpha=0.95, beta=0.05)\n",
    "                loss = loss / accumulate_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulate_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']:.6f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65066da3-67d1-4e79-9234-f7a748852240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + residual)\n",
    "\n",
    "# UNet with Adaptive Pooling\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                          kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=32)\n",
    "        self.enc1_2 = CBR2d(in_channels=32, out_channels=32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=32, out_channels=64)\n",
    "        self.enc2_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
    "\n",
    "        self.enc3_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc3_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        self.pool3 = nn.AdaptiveMaxPool2d((10, 21))\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc4_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)\n",
    "\n",
    "        self.enc5_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc5_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        self.pool5 = nn.AdaptiveMaxPool2d((5, 11))\n",
    "\n",
    "        self.enc6_1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024)\n",
    "        )\n",
    "\n",
    "        # Expansive path\n",
    "        self.dec6_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "        self.unpool5 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=(4, 2), padding=0)\n",
    "\n",
    "        self.dec5_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec5_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=(3, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec4_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=(2, 3), stride=(2, 2), padding=0)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec3_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec2_1 = CBR2d(in_channels=64, out_channels=32)\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "\n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 32, out_channels=32)\n",
    "        self.dec1_1 = CBR2d(in_channels=32, out_channels=32)\n",
    "\n",
    "        self.fc = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting path\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "    \n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "    \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "    \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "    \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        enc5_2 = self.enc5_2(enc5_1)\n",
    "        pool5 = self.pool5(enc5_2)\n",
    "    \n",
    "        enc6_1 = self.enc6_1(pool5)\n",
    "    \n",
    "        # Expansive path\n",
    "        dec6_1 = self.dec6_1(enc6_1)\n",
    "        unpool5 = self.unpool5(dec6_1)\n",
    "\n",
    "        diffY = unpool5.size()[2] - enc5_2.size()[2]\n",
    "        diffX = unpool5.size()[3] - enc5_2.size()[3]\n",
    "    \n",
    "        if diffY != 0 or diffX != 0:\n",
    "            enc5_2_padded = F.pad(enc5_2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        else:\n",
    "            enc5_2_padded = enc5_2\n",
    "    \n",
    "        cat5 = torch.cat((unpool5, enc5_2_padded), dim=1)\n",
    "        dec5_2 = self.dec5_2(cat5)\n",
    "        dec5_1 = self.dec5_1(dec5_2)\n",
    "    \n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "\n",
    "        diffY = unpool4.size()[2] - enc4_2.size()[2]\n",
    "        diffX = unpool4.size()[3] - enc4_2.size()[3]\n",
    "    \n",
    "        if diffY != 0 or diffX != 0:\n",
    "            enc4_2_padded = F.pad(enc4_2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        else:\n",
    "            enc4_2_padded = enc4_2\n",
    "    \n",
    "        cat4 = torch.cat((unpool4, enc4_2_padded), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "    \n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "\n",
    "        diffY = unpool3.size()[2] - enc3_2.size()[2]\n",
    "        diffX = unpool3.size()[3] - enc3_2.size()[3]\n",
    "    \n",
    "        if diffY != 0 or diffX != 0:\n",
    "            enc3_2_padded = F.pad(enc3_2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        else:\n",
    "            enc3_2_padded = enc3_2\n",
    "    \n",
    "        cat3 = torch.cat((unpool3, enc3_2_padded), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "    \n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "\n",
    "        diffY = unpool2.size()[2] - enc2_2.size()[2]\n",
    "        diffX = unpool2.size()[3] - enc2_2.size()[3]\n",
    "    \n",
    "        if diffY != 0 or diffX != 0:\n",
    "            enc2_2_padded = F.pad(enc2_2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        else:\n",
    "            enc2_2_padded = enc2_2\n",
    "    \n",
    "        cat2 = torch.cat((unpool2, enc2_2_padded), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "    \n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "\n",
    "        diffY = unpool1.size()[2] - enc1_2.size()[2]\n",
    "        diffX = unpool1.size()[3] - enc1_2.size()[3]\n",
    "    \n",
    "        if diffY != 0 or diffX != 0:\n",
    "            enc1_2_padded = F.pad(enc1_2, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        else:\n",
    "            enc1_2_padded = enc1_2\n",
    "    \n",
    "        cat1 = torch.cat((unpool1, enc1_2_padded), dim=1)\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        x = self.fc(dec1_1)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d85c4f-7688-4812-a99c-5b44192cc2c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "model.apply(weights_init)\n",
    "summary(model, input_size=(3, 35, 85))\n",
    "\n",
    "criterion = lambda pred, target: dual_loss(pred, target, freq_mask, alpha=0.95, beta=0.05)\n",
    "learning_rate = 0.0002842422846893456\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc06d48-0306-4bd3-ad8a-e1168b0658c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tensor, tensor_name=\"Tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN values detected in {tensor_name}.\")\n",
    "    else:\n",
    "        print(f\"No NaN values detected in {tensor_name}.\")\n",
    "\n",
    "check_nan(train_dataset.inputs, \"train_dataset.inputs\")\n",
    "\n",
    "check_nan(val_dataset.inputs, \"val_dataset.inputs\")\n",
    "\n",
    "check_nan(test_dataset.inputs, \"test_dataset.inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b66e0d-edc8-409a-a771-6f40edc55cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, accumulate_steps=4, device=device)\n",
    "\n",
    "# Test the model\n",
    "predictions, true_labels = test_model(model, test_loader)\n",
    "\n",
    "# Print or save the test results as needed\n",
    "print(\"Test predictions and true labels have been computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5241c-b293-4196-88b8-015c2e04fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hycom_grid_fn_path = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/HYCOM_grid_info.mat'\n",
    "with h5py.File(hycom_grid_fn_path, 'r') as mat_file:\n",
    "    hycom_lon = mat_file['lon'][:]\n",
    "    hycom_lat = mat_file['lat'][:]\n",
    "\n",
    "roms_grid_fn_path = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/roms_grid_info.mat'\n",
    "with h5py.File(roms_grid_fn_path, 'r') as mat_file:\n",
    "    roms_lon = mat_file['lon_rho'][:]\n",
    "    roms_lat = mat_file['lat_rho'][:]\n",
    "\n",
    "def visualize_with_lat_lon(lat, lon, true_velocity, predicted_velocity, sample_idx=0):\n",
    "    \"\"\"\n",
    "    lat: latitude [height]\n",
    "    lon: longitude [width]\n",
    "    true_vorticity: Ground truth vorticity [height, width]\n",
    "    predicted_vorticity: Predicted vorticity [height, width]\n",
    "    sample_idx: the index of the sample to visualize (not used here)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Selecting the sample to visualize (you can loop over multiple samples if needed)\n",
    "    true_velocity = true_velocity[sample_idx].squeeze()  # Ground truth velocity [height, width]\n",
    "    predicted_velocity = predicted_velocity[sample_idx].squeeze()  # Predicted velocity [height, width]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Ground truth vorticity plot\n",
    "    im1 = axes[0].pcolormesh(lon, lat, true_velocity, cmap='seismic', vmin=-0.5, vmax=0.5)\n",
    "    axes[0].set_title('Ground Truth Velocity')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    # Predicted vorticity plot\n",
    "    im2 = axes[1].pcolormesh(lon, lat, predicted_velocity, cmap='seismic', vmin=-0.5, vmax=0.5)\n",
    "    axes[1].set_title('Predicted Velocity')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# figure u-velocity\n",
    "visualize_with_lat_lon(roms_lat, roms_lon, true_labels[:,0,:,:], predictions[:,0,:,:], sample_idx=100)\n",
    "visualize_with_lat_lon(roms_lat, roms_lon, true_labels[:,1,:,:], predictions[:,1,:,:], sample_idx=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ee11e-9652-482f-bbcd-42d8972fcd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_bandpassed_comparison(pred, target, freq_mask, vmin=-0.5, vmax=0.5, cmap='RdBu_r'):\n",
    "    \"\"\"\n",
    "    pred, target: [1, 1, H, W] or [C, H, W]\n",
    "    freq_mask: [H, W]\n",
    "    \"\"\"\n",
    "    if pred.ndim == 4:\n",
    "        pred = pred[0, 0]\n",
    "    elif pred.ndim == 3:\n",
    "        pred = pred[0]\n",
    "\n",
    "    if target.ndim == 4:\n",
    "        target = target[0, 0]\n",
    "    elif target.ndim == 3:\n",
    "        target = target[0]\n",
    "\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "    freq_mask = freq_mask.to(pred.device)\n",
    "\n",
    "    pred_dct = dct_2d(pred)\n",
    "    target_dct = dct_2d(target)\n",
    "\n",
    "    band_mask = freq_mask.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "\n",
    "    pred_filtered = idct_2d(pred_dct * band_mask)\n",
    "    target_filtered = idct_2d(target_dct * band_mask)\n",
    "\n",
    "    pred_np = pred_filtered.detach().cpu().squeeze().numpy()\n",
    "    target_np = target_filtered.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    im0 = axs[0].imshow(target_np, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axs[0].set_title(\"Ground Truth (bandpassed)\")\n",
    "    plt.colorbar(im0, ax=axs[0])\n",
    "\n",
    "    im1 = axs[1].imshow(pred_np, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axs[1].set_title(\"Prediction (bandpassed)\")\n",
    "    plt.colorbar(im1, ax=axs[1])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b1aed-0d1e-4245-815e-b7c92619c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 500\n",
    "pred=torch.tensor(predictions[tt,0,:,:])\n",
    "target=torch.tensor(true_labels[tt,0,:,:])\n",
    "\n",
    "plot_bandpassed_comparison(pred, target, freq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2514a1-f7d5-422d-b2ab-50f771718d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "with h5py.File('UNet_test_250411_except_MLD_temp_sorted_real_5km_20km_optimized_output.mat', 'w') as f:\n",
    "    f.create_dataset('true_labels', data=true_labels)\n",
    "    f.create_dataset('predictions', data=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1550a1-c724-4b3d-a65b-9cf30c01e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('UNet_test_250411_except_MLD_temp_sorted_real_5km_20km_optimized_roms_grid_info.mat', 'w') as f2:\n",
    "    f2.create_dataset('roms_lon', data=roms_lon)\n",
    "    f2.create_dataset('roms_lat', data=roms_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe62387-9aea-4297-90f5-f462bb94bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'UNet_test_250411_except_MLD_temp_sorted_real_5km_20km_optimized.pth')\n",
    "torch.save(model.state_dict(), 'UNet_test_250411_except_MLD_temp_sorted_real_5km_20km_optimized_state_dict.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
