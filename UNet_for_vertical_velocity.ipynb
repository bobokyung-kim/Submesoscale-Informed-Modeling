{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32142f-5e6a-472f-b6aa-683e458b7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b8b24-0255-421d-863d-3e006176261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import torchvision.transforms.functional as TF\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loading functions\n",
    "def load_data_from_mat(filename, u_v_names=None, w_name=None, mld_name=None):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        if u_v_names:\n",
    "            u_velocity = torch.tensor(np.array(f[u_v_names[0]])).float()  # [t, n, m]\n",
    "            v_velocity = torch.tensor(np.array(f[u_v_names[1]])).float()  # [t, n, m]\n",
    "        else:\n",
    "            u_velocity = v_velocity = None\n",
    "\n",
    "        if w_name:\n",
    "            w_velocity = torch.tensor(np.array(f[w_name])).squeeze().float()  # [t]\n",
    "        else:\n",
    "            w_velocity = None\n",
    "\n",
    "        if mld_name:\n",
    "            mld = torch.tensor(np.array(f[mld_name])).squeeze().float()  # [t]\n",
    "        else:\n",
    "            mld = None\n",
    "            \n",
    "    return u_velocity, v_velocity, w_velocity, mld\n",
    "    \n",
    "def load_multiple_h5py_data(u_v_files_pattern, u_v_names):\n",
    "    u_velocity_list, v_velocity_list = [], []\n",
    "    u_v_files = glob.glob(u_v_files_pattern)\n",
    "    u_v_files = sorted(u_v_files, key=lambda x: [int(num) for num in re.findall(r'\\d+', x)])\n",
    "    \n",
    "    for file in u_v_files:\n",
    "        print(file)\n",
    "        u_velocity, v_velocity = load_data_from_mat(file, u_v_names, None)[:2]\n",
    "        u_velocity_list.append(u_velocity)\n",
    "        v_velocity_list.append(v_velocity)\n",
    "\n",
    "    u_velocity = torch.cat(u_velocity_list, dim=0)  # [t_total, n, m]\n",
    "    v_velocity = torch.cat(v_velocity_list, dim=0)  # [t_total, n, m]\n",
    "\n",
    "    return u_velocity, v_velocity\n",
    "\n",
    "def load_multiple_w_data(w_files_pattern, w_name):\n",
    "    w_velocity_list = []\n",
    "    w_files = glob.glob(w_files_pattern)\n",
    "\n",
    "    w_files = sorted(w_files, key=lambda x: [int(num) for num in re.findall(r'\\d+', x)])\n",
    "    \n",
    "    for file in w_files:\n",
    "        print(file)\n",
    "        _, _, w_velocity, _ = load_data_from_mat(file, None, w_name)\n",
    "        w_velocity_list.append(w_velocity)\n",
    "\n",
    "    w_velocity = torch.cat(w_velocity_list, dim=0)  # [t_total, n, m]\n",
    "\n",
    "    return w_velocity\n",
    "\n",
    "def load_multiple_mld_data(mld_files_pattern, mld_name):\n",
    "    mld_list = []\n",
    "    mld_files = glob.glob(mld_files_pattern)\n",
    "    mld_files = sorted(mld_files, key=lambda x: [int(num) for num in re.findall(r'\\d+', x)])\n",
    "    for file in mld_files:\n",
    "        print(file)\n",
    "        _, _, _, mld = load_data_from_mat(file, None, None, mld_name)\n",
    "        if mld is not None:\n",
    "            mld_list.append(mld)\n",
    "    if len(mld_list) == 0:\n",
    "        raise ValueError(\"No MLD data found.\")\n",
    "    mld = torch.cat(mld_list, dim=0)  # [t_total]\n",
    "    return mld\n",
    "    \n",
    "def calculate_stats(tensor):\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    mean = np.nanmean(tensor_np, axis=(0, 2, 3), keepdims=True)\n",
    "    std = np.nanstd(tensor_np, axis=(0, 2, 3), keepdims=True)\n",
    "    return torch.tensor(mean).to(tensor.device), torch.tensor(std).to(tensor.device)\n",
    "\n",
    "def normalize(tensor, mean, std):\n",
    "    return (tensor - mean) / (std + 1e-5)\n",
    "\n",
    "class LRVelocityMLDDataset(Dataset):\n",
    "    def __init__(self, high_u_velocity, high_v_velocity, w_velocity, mld, mean, std):\n",
    "        self.high_u_velocity = torch.nan_to_num(high_u_velocity)  # [t, n, m]\n",
    "        self.high_v_velocity = torch.nan_to_num(high_v_velocity)  # [t, n, m]\n",
    "        self.w_velocity = torch.nan_to_num(w_velocity)  # [t, n, m]\n",
    "        self.mld = torch.nan_to_num(mld)                # [t]\n",
    "\n",
    "        if self.mld.dim() == 1:\n",
    "            t = self.mld.shape[0]\n",
    "            n, m = self.high_u_velocity.shape[1], self.high_u_velocity.shape[2]\n",
    "            self.mld = self.mld[:, None, None].repeat(1, n, m)  # [t,n,m]\n",
    "        \n",
    "        self.inputs = torch.stack([self.high_u_velocity, self.high_v_velocity, self.mld], dim=1)  # [t, 3, n, m]\n",
    "        self.outputs = torch.stack([self.w_velocity], dim=1)  # [t, 1, n, m]\n",
    "        \n",
    "        self.inputs = normalize(self.inputs, mean, std)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.inputs[idx, :, :, :]                  # [3, n, m]\n",
    "        output_data = self.outputs[idx, :, :, :]                  # [1, n, m]\n",
    "        return input_data, output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ec0d2-f3b3-4a3b-a9fb-afb2e684bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEMO_MODE:\n",
    "    high_u_v_names_train = ['BBE_surface_u_rho_points', 'BBE_surface_v_rho_points']\n",
    "    w_name_train = 'BBE_surface_w'\n",
    "    mld_name_train = 'mld'\n",
    "    \n",
    "    train_high_u_velocity, train_high_v_velocity = load_multiple_h5py_data(\n",
    "        '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/ROMS_surface_velocity_rho_points_2021_*.mat',\n",
    "        high_u_v_names_train\n",
    "    )\n",
    "    print(f\"train_high_u_velocity shape: {train_high_u_velocity.shape}\")  # [t_total]\n",
    "    print(f\"train_high_v_velocity shape: {train_high_v_velocity.shape}\")  # [t_total]\n",
    "    \n",
    "    w_files_pattern_train = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/ROMS_surface_vertical_velocity_2021_*.mat'\n",
    "    train_w_velocity = load_multiple_w_data(w_files_pattern_train, w_name_train)\n",
    "    print(f\"train_w shape: {train_w_velocity.shape}\")  # [t_total]\n",
    "    \n",
    "    mld_files_pattern_train = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn_final/roms_temp_2021_*.mat'\n",
    "    train_MLD = load_multiple_mld_data(mld_files_pattern_train, mld_name_train)\n",
    "    print(f\"train_mld shape: {train_MLD.shape}\")  # [t_total]\n",
    "    \n",
    "    assert train_high_u_velocity.shape[0] == train_high_v_velocity.shape[0] == train_w_velocity.shape[0], \\\n",
    "        \"All input tensors must have the same number of samples (t dimension).\"\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    \n",
    "    num_hours_per_month = [744, 672, 744, 720, 744, 720, 744, 744, 720, 744, 720, 744]\n",
    "    cumulative_hours = np.cumsum([0] + num_hours_per_month)\n",
    "    \n",
    "    train_u, val_u, test_u = [], [], []\n",
    "    train_v, val_v, test_v = [], [], []\n",
    "    train_w, val_w, test_w = [], [], []\n",
    "    train_mld, val_mld, test_mld = [], [], []\n",
    "    \n",
    "    for i in range(12):\n",
    "        start, end = cumulative_hours[i], cumulative_hours[i+1]\n",
    "    \n",
    "        u_month = train_high_u_velocity[start:end]\n",
    "        v_month = train_high_v_velocity[start:end]\n",
    "        w_month = train_w_velocity[start:end]\n",
    "        mld_month = train_MLD[start:end]\n",
    "    \n",
    "        all_data = list(zip(u_month, v_month, mld_month, w_month))\n",
    "    \n",
    "        indices = np.arange(len(all_data))\n",
    "        \n",
    "        train_idx, test_idx = train_test_split(\n",
    "            indices, test_size=0.3, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"Train indices:\", train_idx)\n",
    "        print(\"Test indices:\", test_idx)\n",
    "    \n",
    "    num_hours_per_month = [744, 672, 744, 720, 744, 720, 744, 744, 720, 744, 720, 744]\n",
    "    cumulative_hours = np.cumsum([0] + num_hours_per_month)\n",
    "    \n",
    "    train_u, val_u, test_u = [], [], []\n",
    "    train_v, val_v, test_v = [], [], []\n",
    "    train_w, val_w, test_w = [], [], []\n",
    "    train_mld, val_mld, test_mld = [], [], []\n",
    "    \n",
    "    for i in range(12):\n",
    "        start, end = cumulative_hours[i], cumulative_hours[i+1]\n",
    "    \n",
    "        u_month = train_high_u_velocity[start:end]\n",
    "        v_month = train_high_v_velocity[start:end]\n",
    "        w_month = train_w_velocity[start:end]\n",
    "        mld_month = train_MLD[start:end]\n",
    "    \n",
    "        all_data = list(zip(u_month, v_month, mld_month, w_month))\n",
    "    \n",
    "        train_set, val_test_set = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "    \n",
    "        val_set, test_set = train_test_split(val_test_set, test_size=0.5, random_state=42)\n",
    "    \n",
    "        u_train, v_train, mld_train, w_train = zip(*train_set)\n",
    "        u_val, v_val, mld_val, w_val = zip(*val_set)\n",
    "        u_test, v_test, mld_test, w_test = zip(*test_set)\n",
    "    \n",
    "        train_u.append(np.stack(u_train))\n",
    "        val_u.append(np.stack(u_val))\n",
    "        test_u.append(np.stack(u_test))\n",
    "    \n",
    "        train_v.append(np.stack(v_train))\n",
    "        val_v.append(np.stack(v_val))\n",
    "        test_v.append(np.stack(v_test))\n",
    "    \n",
    "        train_mld.append(np.stack(mld_train))\n",
    "        val_mld.append(np.stack(mld_val))\n",
    "        test_mld.append(np.stack(mld_test))\n",
    "    \n",
    "        train_w.append(np.stack(w_train))\n",
    "        val_w.append(np.stack(w_val))\n",
    "        test_w.append(np.stack(w_test))\n",
    "    \n",
    "    inputs_high_u_train = np.concatenate(train_u)\n",
    "    inputs_high_u_val = np.concatenate(val_u)\n",
    "    inputs_high_u_test = np.concatenate(test_u)\n",
    "    \n",
    "    inputs_high_v_train = np.concatenate(train_v)\n",
    "    inputs_high_v_val = np.concatenate(val_v)\n",
    "    inputs_high_v_test = np.concatenate(test_v)\n",
    "    \n",
    "    outputs_w_train = np.concatenate(train_w)\n",
    "    outputs_w_val = np.concatenate(val_w)\n",
    "    outputs_w_test = np.concatenate(test_w)\n",
    "    \n",
    "    inputs_mld_train = np.concatenate(train_mld)\n",
    "    inputs_mld_val = np.concatenate(val_mld)\n",
    "    inputs_mld_test = np.concatenate(test_mld)\n",
    "    \n",
    "    print(f\"Train size: {len(inputs_high_u_train)} ({len(inputs_high_u_train) / 8760 * 100:.1f}%)\")\n",
    "    print(f\"Validation size: {len(inputs_high_u_val)} ({len(inputs_high_u_val) / 8760 * 100:.1f}%)\")\n",
    "    print(f\"Test size: {len(inputs_high_u_test)} ({len(inputs_high_u_test) / 8760 * 100:.1f}%)\")\n",
    "    \n",
    "    inputs_high_u_train = torch.from_numpy(inputs_high_u_train).float()\n",
    "    inputs_high_u_val = torch.from_numpy(inputs_high_u_val).float()\n",
    "    inputs_high_u_test = torch.from_numpy(inputs_high_u_test).float()\n",
    "    \n",
    "    inputs_high_v_train = torch.from_numpy(inputs_high_v_train).float()\n",
    "    inputs_high_v_val = torch.from_numpy(inputs_high_v_val).float()\n",
    "    inputs_high_v_test = torch.from_numpy(inputs_high_v_test).float()\n",
    "    \n",
    "    outputs_w_train = torch.from_numpy(outputs_w_train).float()\n",
    "    outputs_w_val = torch.from_numpy(outputs_w_val).float()\n",
    "    outputs_w_test = torch.from_numpy(outputs_w_test).float()\n",
    "    \n",
    "    inputs_mld_train = torch.from_numpy(inputs_mld_train).float()\n",
    "    inputs_mld_val = torch.from_numpy(inputs_mld_val).float()\n",
    "    inputs_mld_test = torch.from_numpy(inputs_mld_test).float()\n",
    "    \n",
    "    if inputs_mld_train.dim() == 1:\n",
    "        n, m = inputs_high_u_train.shape[1], inputs_high_u_train.shape[2]\n",
    "        inputs_mld_train_map = inputs_mld_train[:, None, None].repeat(1, n, m)\n",
    "    else:\n",
    "        inputs_mld_train_map = inputs_mld_train\n",
    "        \n",
    "    train_mean, train_std = calculate_stats(torch.stack([inputs_high_u_train, inputs_high_v_train, inputs_mld_train], dim=1))\n",
    "    \n",
    "    train_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_train,\n",
    "        high_v_velocity=inputs_high_v_train,\n",
    "        mld=inputs_mld_train,\n",
    "        w_velocity=outputs_w_train * 1000.0,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "    \n",
    "    val_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_val,\n",
    "        high_v_velocity=inputs_high_v_val,\n",
    "        mld=inputs_mld_val,\n",
    "        w_velocity=outputs_w_val * 1000.0,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "    \n",
    "    test_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_test,\n",
    "        high_v_velocity=inputs_high_v_test,\n",
    "        mld=inputs_mld_test,\n",
    "        w_velocity=outputs_w_test * 1000.0,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "else:\n",
    "    print(\"DEMO MODE: using dummy tensors (no external data files).\")\n",
    "\n",
    "    t_train, t_val, t_test = 16, 8, 8\n",
    "    n, m = 280, 340\n",
    "\n",
    "    # ---- dummy inputs ----\n",
    "    inputs_high_u_train = torch.randn(t_train, n, m)\n",
    "    inputs_high_u_val   = torch.randn(t_val, n, m)\n",
    "    inputs_high_u_test  = torch.randn(t_test, n, m)\n",
    "\n",
    "    inputs_high_v_train = torch.randn(t_train, n, m)\n",
    "    inputs_high_v_val   = torch.randn(t_val, n, m)\n",
    "    inputs_high_v_test  = torch.randn(t_test, n, m)\n",
    "\n",
    "    # w target (note: your original code multiplies by 1000.0; keep that if you want)\n",
    "    outputs_w_train = torch.randn(t_train, n, m) * 1000.0\n",
    "    outputs_w_val   = torch.randn(t_val, n, m) * 1000.0\n",
    "    outputs_w_test  = torch.randn(t_test, n, m) * 1000.0\n",
    "\n",
    "    # mld is originally [t]; keep it 1D here and broadcast inside Dataset (see section 2 below)\n",
    "    inputs_mld_train = torch.randn(t_train)\n",
    "    inputs_mld_val   = torch.randn(t_val)\n",
    "    inputs_mld_test  = torch.randn(t_test)\n",
    "\n",
    "    # ---- IMPORTANT: compute stats using broadcasted mld map ----\n",
    "    mld_train_map = inputs_mld_train[:, None, None].repeat(1, n, m)  # [t,n,m]\n",
    "    train_mean, train_std = calculate_stats(\n",
    "        torch.stack([inputs_high_u_train, inputs_high_v_train, mld_train_map], dim=1)\n",
    "    )\n",
    "\n",
    "    train_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_train,\n",
    "        high_v_velocity=inputs_high_v_train,\n",
    "        mld=inputs_mld_train,          # 1D ok if Dataset broadcasts\n",
    "        w_velocity=outputs_w_train,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "\n",
    "    val_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_val,\n",
    "        high_v_velocity=inputs_high_v_val,\n",
    "        mld=inputs_mld_val,\n",
    "        w_velocity=outputs_w_val,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "\n",
    "    test_dataset = LRVelocityMLDDataset(\n",
    "        high_u_velocity=inputs_high_u_test,\n",
    "        high_v_velocity=inputs_high_v_test,\n",
    "        mld=inputs_mld_test,\n",
    "        w_velocity=outputs_w_test,\n",
    "        mean=train_mean,\n",
    "        std=train_std\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99465fd0-e9b1-4e1b-b33b-04b62ee04eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses)+1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60429cb-42e0-44e7-85cf-faa48862d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for inputs, outputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                p_outputs = model(inputs)\n",
    "                predictions.append(p_outputs.cpu().numpy())  # Store predictions\n",
    "                true_labels.append(outputs.cpu().numpy())  # Store true labels\n",
    "\n",
    "    # Concatenate all predictions and true labels\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56902e-c290-46e3-887a-0599563fbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=200, accumulate_steps=4, device='cuda', scheduler=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / accumulate_steps  # Gradients Accumulation\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulate_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']:.6f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65066da3-67d1-4e79-9234-f7a748852240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Conv → BatchNorm → ReLU) × 2\n",
    "def double_conv(in_channels, out_channels, dropout_rate=0.0):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        \n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    ]\n",
    "    if dropout_rate > 0:\n",
    "        layers.insert(2, nn.Dropout2d(dropout_rate))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 features=[64, 128, 256, 512],\n",
    "                 dropout_rate=0.0):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.downs = nn.ModuleList()\n",
    "        current_in_channels = in_channels\n",
    "        for feature in features:\n",
    "            self.downs.append(double_conv(current_in_channels, feature, dropout_rate))\n",
    "            current_in_channels = feature\n",
    "\n",
    "        self.bottleneck = double_conv(features[-1], features[-1]*2, dropout_rate)\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(in_channels=feature*2, \n",
    "                                   out_channels=feature,\n",
    "                                   kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(\n",
    "                double_conv(feature*2, feature, dropout_rate)\n",
    "            )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down_block in self.downs:\n",
    "            x = down_block(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:], \n",
    "                                  mode='bilinear', align_corners=False)\n",
    "\n",
    "            x = torch.cat((skip_connection, x), dim=1)\n",
    "\n",
    "            x = self.ups[idx + 1](x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d85c4f-7688-4812-a99c-5b44192cc2c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "model.apply(weights_init)\n",
    "summary(model, input_size=(3, 280, 340))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.00022608954273243165\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc06d48-0306-4bd3-ad8a-e1168b0658c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(tensor, tensor_name=\"Tensor\"):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN values detected in {tensor_name}.\")\n",
    "    else:\n",
    "        print(f\"No NaN values detected in {tensor_name}.\")\n",
    "\n",
    "check_nan(train_dataset.inputs, \"train_dataset.inputs\")\n",
    "\n",
    "check_nan(val_dataset.inputs, \"val_dataset.inputs\")\n",
    "\n",
    "check_nan(test_dataset.inputs, \"test_dataset.inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b66e0d-edc8-409a-a771-6f40edc55cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, accumulate_steps=4, device=device)\n",
    "\n",
    "# Test the model\n",
    "predictions, true_labels = test_model(model, test_loader)\n",
    "\n",
    "# Print or save the test results as needed\n",
    "print(\"Test predictions and true labels have been computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5241c-b293-4196-88b8-015c2e04fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hycom_grid_fn_path = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/HYCOM_grid_info.mat'\n",
    "with h5py.File(hycom_grid_fn_path, 'r') as mat_file:\n",
    "    hycom_lon = mat_file['lon'][:]\n",
    "    hycom_lat = mat_file['lat'][:]\n",
    "\n",
    "roms_grid_fn_path = '/home/user_bk/roms_ccs/ver4_rotate/data_for_cnn/roms_grid_info.mat'\n",
    "with h5py.File(roms_grid_fn_path, 'r') as mat_file:\n",
    "    roms_lon = mat_file['lon_rho'][:]\n",
    "    roms_lat = mat_file['lat_rho'][:]\n",
    "\n",
    "def visualize_with_lat_lon(lat, lon, true_velocity, predicted_velocity, sample_idx=0):\n",
    "    \"\"\"\n",
    "    lat: latitude [height]\n",
    "    lon: longitude [width]\n",
    "    true_vorticity: Ground truth vorticity [height, width]\n",
    "    predicted_vorticity: Predicted vorticity [height, width]\n",
    "    sample_idx: the index of the sample to visualize (not used here)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Selecting the sample to visualize (you can loop over multiple samples if needed)\n",
    "    true_velocity = true_velocity[sample_idx].squeeze()  # Ground truth velocity [height, width]\n",
    "    predicted_velocity = predicted_velocity[sample_idx].squeeze()  # Predicted velocity [height, width]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Ground truth vorticity plot\n",
    "    im1 = axes[0].pcolormesh(lon, lat, true_velocity, cmap='seismic', vmin=-0.0002, vmax=0.0002)\n",
    "    axes[0].set_title('Ground Truth Velocity')\n",
    "    axes[0].set_xlabel('Longitude')\n",
    "    axes[0].set_ylabel('Latitude')\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "\n",
    "    # Predicted vorticity plot\n",
    "    im2 = axes[1].pcolormesh(lon, lat, predicted_velocity, cmap='seismic', vmin=-0.0002, vmax=0.0002)\n",
    "    axes[1].set_title('Predicted Velocity')\n",
    "    axes[1].set_xlabel('Longitude')\n",
    "    axes[1].set_ylabel('Latitude')\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# figure u-velocity\n",
    "visualize_with_lat_lon(roms_lat, roms_lon, true_labels[:,0,:,:] / 1000, predictions[:,0,:,:] / 1000, sample_idx=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2514a1-f7d5-422d-b2ab-50f771718d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "with h5py.File('UNet_test_250416_uvmld_to_w_dropout_optimized_output.mat', 'w') as f:\n",
    "    f.create_dataset('true_labels', data=true_labels)\n",
    "    f.create_dataset('predictions', data=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1550a1-c724-4b3d-a65b-9cf30c01e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('UNet_test_250416_uvmld_to_w_dropout_optimized_roms_grid_info.mat', 'w') as f2:\n",
    "    f2.create_dataset('roms_lon', data=roms_lon)\n",
    "    f2.create_dataset('roms_lat', data=roms_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe62387-9aea-4297-90f5-f462bb94bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'UNet_test_250416_uvmld_to_w_optimized_dropout.pth')\n",
    "torch.save(model.state_dict(), 'UNet_test_250416_uvmld_to_w_optimized_dropout_state_dict.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
